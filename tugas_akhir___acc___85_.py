# -*- coding: utf-8 -*-
"""Tugas Akhir | acc > 85%.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xaNvB1CP6CUQgrYKb_6MTS6NVbqpRlXm

#Import Library
"""

!pip install pydotplus

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as ss
# %matplotlib inline
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler, StandardScaler,RobustScaler
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.compose import ColumnTransformer
from sklearn.metrics import auc,roc_curve, roc_auc_score, confusion_matrix, accuracy_score, classification_report,precision_score, recall_score, RocCurveDisplay, f1_score, roc_auc_score,mean_squared_error
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier
from xgboost import XGBClassifier, plot_importance
from sklearn.decomposition import PCA
from sklearn.preprocessing import label_binarize
from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus

"""#Import Data"""

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/Dataset TA/LoanData_Bondora.csv")
data

data['NrOfDependants'] = pd.to_numeric(data['NrOfDependants'], errors='coerce')
data['NrOfDependants'] = data['NrOfDependants'].astype('Int64')

"""#Features Selection"""

attr=['LoanId','UserName','NrOfDependants','Age','Gender','BidsPortfolioManager','Restructured','UseOfLoan','VerificationType','EmploymentStatus','DebtToIncome','HomeOwnershipType','AppliedAmount','IncomeTotal','Rating_V2']
df = data[attr]
df.dtypes

names=df.columns
names

"""#Drop Column Which Have 50% Null"""

list_of_50_percent_null = [ ]
for i in df.columns:
    if df[i].isnull().sum() >= (90*179235)/100:
        list_of_50_percent_null.append(i)

list_of_50_percent_null

"""#Separate Columns Which Have Object Data"""

obj_data=df.select_dtypes('object')
df = df.drop(obj_data.columns, axis=1)

obj_data

"""#Seperate Columns Which Have Boolean Data"""

bool_data = df.select_dtypes('bool')
df = df.drop(bool_data.columns, axis=1)

bool_data

"""#Fill null data with 'Unknown'"""

obj_data=obj_data.fillna('unknown')

"""#Labelling Object and Boolean Data"""

obj_array= np.array(obj_data).reshape(-1)
bool_array= np.array(bool_data).reshape(-1)

obj_array

encoder_1= LabelEncoder()
encoder_2= LabelEncoder()

obj_enc= encoder_1.fit_transform(obj_array)
bool_enc= encoder_2.fit_transform(bool_array)

obj_enc= pd.DataFrame(obj_enc.reshape(179235,int(obj_enc.shape[0]/179235)))
bool_enc= pd.DataFrame(bool_enc.reshape(179235,int(bool_enc.shape[0]/179235)))

features_obj_data= list(obj_data.columns)
features_bool_data= list(bool_data.columns)

obj_enc.columns= features_obj_data
bool_enc.columns= features_bool_data

"""#Concatenate Object and Boolean Data"""

object_data=pd.concat([obj_enc,bool_enc], axis=1)
object_data

df

"""#Transform Data with Median Value using Imputer"""

names_num = df.columns
imp_median = SimpleImputer(missing_values=np.nan, strategy='median')
imp_median.fit(df)
imp_data= imp_median.transform(df)
df= pd.DataFrame(imp_data)
df.columns= names_num

df

df.describe().transpose()

"""#Normalize Numeric Data using Z Score Normalizer"""

def z_score_normalizer(X):
    m = X.shape[0]
    n = 1
    for i in range(n):
        X = (X - X.mean(axis=0))/X.std(axis=0)
    return X

df = z_score_normalizer(df)

"""#Target Selection"""

Y= object_data['Rating_V2']
object_data = object_data.drop(['Rating_V2'],axis=1)
Y= encoder_1.inverse_transform(Y)
Y= pd.DataFrame(Y, columns=['Rating_V2'])
Y.Rating_V2.unique()

Y= Y.loc[Y.Rating_V2!='unknown']
Y= Y.replace(['C', 'B', 'A', 'HR', 'F', 'D', 'E', 'AA'],[0,1,2,3,4,5,6,7])
# Y= Y.astype(float)
# mean_value = np.nanmean(Y.Rating_V2)
# Y= Y.fillna(mean_value)
Y

Y.isnull().values.any()

"""#Concatenate Boolean, Object, Numerical Data"""

all_data = pd.concat([object_data,df,Y], axis=1)
column_means = np.nanmean(all_data['Rating_V2'], axis=0)
missing_indices = np.isnan(all_data['Rating_V2'])
all_data[missing_indices]= np.take(column_means, missing_indices[1])
all_data['Rating_V2'] = pd.to_numeric(all_data['Rating_V2'], errors='coerce')
all_data['Rating_V2'] = all_data['Rating_V2'].round().astype('Int64')
all_data
# all_data = all_data.dropna()

# all_data.to_csv('all_data.csv',index=False)
# all_data['Rating_V2']

all_data.isnull().values.any()

Y= all_data["Rating_V2"]
X= all_data.drop(["Rating_V2"],axis= 1)

"""#Modelling Data with 80% Data Training & 20% Data Testing"""

# X_train, X2, Y_train, Y2 = train_test_split(X, Y, test_size=0.2, random_state=24)

"""#Modelling Data with 70% Data Training & 30% Data Testing"""

X_train, X2, Y_train, Y2 = train_test_split(X, Y, test_size=0.3, random_state=24)

"""#Modelling Data With 60% Data Training & 40% Data Testing"""

# X_train, X2, Y_train, Y2 = train_test_split(X, Y, test_size=0.4, random_state=24)

sc = StandardScaler()
X_train=sc.fit_transform(X_train)
X2=sc.fit_transform(X2)

nb=GaussianNB()
nb.fit(X_train, Y_train)

Y_pred = nb.predict(X2)

print("y_test shape:", Y2.shape)

print("y_pred shape:", Y_pred.shape)

print(Y2)

print(Y_pred)

"""#Mean Squared Error"""

mse=mean_squared_error(Y2,Y_pred)
mse

"""#Accuracy Score and Confusion Matrix"""

cm = confusion_matrix(Y2, Y_pred)
ac = accuracy_score(Y2, Y_pred)

ac

cm

print(classification_report(Y2, Y_pred))

Y_gnb_score = nb.predict_proba(X2)
Y_gnb_score

# fpr_gnb, tpr_gnb, thresholds_gnb = roc_curve(Y2, Y_gnb_score[:, 1])

"""#Modelling Using Decision Tree"""

model = DecisionTreeClassifier(random_state=4)
model = model.fit(X_train,Y_train)

Y_pred=model.predict(X2)

ac = accuracy_score(Y2, Y_pred)

ac

cm = confusion_matrix(Y2, Y_pred)

cm

print(classification_report(Y2, Y_pred))

# feature_cols= list(X.columns)
# X.columns=feature_cols

# dot_data = StringIO()
# export_graphviz(model, out_file=dot_data,
#                 filled=True, rounded=True,
#                 special_characters=True,feature_names = feature_cols,class_names=['0', '1', '2', '3', '4', '5', '6', '7'])
# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
# Image(graph.create_png())